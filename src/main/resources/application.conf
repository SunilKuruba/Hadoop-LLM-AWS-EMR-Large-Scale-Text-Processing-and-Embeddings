# application.conf for Hadoop MapReduce program

# Hadoop cluster configuration
hadoop {
  fs.defaultFS = "hdfs://localhost:9000"    # HDFS NameNode URL
  jobtracker = "localhost:54311"             # JobTracker location
  replication = 3                           # HDFS replication factor
  blocksize = 134217728                     # Block size in bytes (128 MB by default)
  mapreduce.input.fileinputformat.split.maxsize =  67108864  # 64 MB
  tokenizer.job.name = "tokenizer"
}

# Mapper and Reducer configuration
mapreduce {
  inputformat = "org.apache.hadoop.mapreduce.lib.input.TextInputFormat"
  outputformat = "org.apache.hadoop.mapreduce.lib.output.TextOutputFormat"
  mapper = "com.example.MyMapper"            # Replace with your custom Mapper class
  reducer = "com.example.MyReducer"          # Replace with your custom Reducer class
  combiner = "com.example.MyCombiner"        # Optional, if using a combiner class
  numreducetasks = 1                         # Number of Reducer tasks
}

# Input and Output paths
io {
  inputdir = "/user/hadoop/input"            # HDFS input directory
  outputdir = "/user/hadoop/output"          # HDFS output directory
}

# Hadoop job parameters
job {
  name = "MyHadoopJob"
  jar = "my-hadoop-job.jar"                  # JAR file containing the MapReduce job
}

# Memory and resource allocation
resources {
  mapmemory = 1024                           # Memory in MB for each Mapper task
  reducememory = 2048                        # Memory in MB for each Reducer task
  maptasks = 4                               # Number of Mapper tasks
  reducetasks = 1                            # Number of Reducer tasks
}

# Custom application parameters
app {
  tokenizer.class = "com.example.Tokenizer"  # Replace with your custom Tokenizer class
  encoding = "UTF-8"                         # Text encoding for input data
  additionalParam = "value"                  # Custom parameters
}
